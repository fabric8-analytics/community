{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset and explore it.\n",
    "\n",
    "For the purposes of this tutorial we will be working with a dump of the NPM registry itself. It was collected using a \"feed-watcher\" for the NPM registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_details = pd.read_json(\n",
    "    \"node-package-details-clean.json\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "\n",
    "First we ignore all packages that have less than 5 dependencies for themselves. Reason for this is otherwise we end up with a very sparse matrix (>99.8%) parsity which leads to garbage weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_details['name'] = node_details.astype('unicode')['name'].str.lower()\n",
    "\n",
    "manifest_rows = node_details[pd.notnull(node_details['dependencies'])]\n",
    "manifest_rows = manifest_rows[manifest_rows['dependencies'].str.len() >= 5]\n",
    "\n",
    "packages_with_keywords = node_details[pd.notnull(node_details['keywords'])]\n",
    "packages_with_keywords = packages_with_keywords[packages_with_keywords['keywords'].str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105818"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifest_rows.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>allDependencies</th>\n",
       "      <th>dependencies</th>\n",
       "      <th>description</th>\n",
       "      <th>devDependencies</th>\n",
       "      <th>keywords</th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[async, connect-redis, express, nodeunit, redi...</td>\n",
       "      <td>[async, connect-redis, express, nodeunit, redi...</td>\n",
       "      <td>awesome framework</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>awesome</td>\n",
       "      <td>0.0.7</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[ejs, express, oauth, socket.io, underscore]</td>\n",
       "      <td>[ejs, express, oauth, socket.io, underscore]</td>\n",
       "      <td>IRC-like chatroom + Twitter live-stream</td>\n",
       "      <td>[]</td>\n",
       "      <td>[real-time, webapp, ExpressJS, socket.io, jQue...</td>\n",
       "      <td>socket-twitchat</td>\n",
       "      <td>0.7.15</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>[async, coffee-script, coloured-log, growl, op...</td>\n",
       "      <td>[async, coffee-script, coloured-log, growl, op...</td>\n",
       "      <td>Kontinuos Integrated Testing Koffee Applicatio...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[integration, testing, framework, expresso, ja...</td>\n",
       "      <td>kitkat</td>\n",
       "      <td>0.3.0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[extx-reference-slot, extx-shotenjin, johnny-m...</td>\n",
       "      <td>[extx-reference-slot, extx-shotenjin, johnny-m...</td>\n",
       "      <td>Framework for web-sites running on the client</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>symbie</td>\n",
       "      <td>0.0.2</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>[bufferjs, html5, jsdom, sharedjs, underscore]</td>\n",
       "      <td>[bufferjs, html5, jsdom, sharedjs, underscore]</td>\n",
       "      <td>Easy way to write a spider.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[spider, scrap]</td>\n",
       "      <td>scrap</td>\n",
       "      <td>1.0.1</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       allDependencies  \\\n",
       "14   [async, connect-redis, express, nodeunit, redi...   \n",
       "22        [ejs, express, oauth, socket.io, underscore]   \n",
       "56   [async, coffee-script, coloured-log, growl, op...   \n",
       "98   [extx-reference-slot, extx-shotenjin, johnny-m...   \n",
       "109     [bufferjs, html5, jsdom, sharedjs, underscore]   \n",
       "\n",
       "                                          dependencies  \\\n",
       "14   [async, connect-redis, express, nodeunit, redi...   \n",
       "22        [ejs, express, oauth, socket.io, underscore]   \n",
       "56   [async, coffee-script, coloured-log, growl, op...   \n",
       "98   [extx-reference-slot, extx-shotenjin, johnny-m...   \n",
       "109     [bufferjs, html5, jsdom, sharedjs, underscore]   \n",
       "\n",
       "                                           description devDependencies  \\\n",
       "14                                   awesome framework              []   \n",
       "22             IRC-like chatroom + Twitter live-stream              []   \n",
       "56   Kontinuos Integrated Testing Koffee Applicatio...              []   \n",
       "98       Framework for web-sites running on the client              []   \n",
       "109                        Easy way to write a spider.              []   \n",
       "\n",
       "                                              keywords             name  \\\n",
       "14                                                 NaN          awesome   \n",
       "22   [real-time, webapp, ExpressJS, socket.io, jQue...  socket-twitchat   \n",
       "56   [integration, testing, framework, expresso, ja...           kitkat   \n",
       "98                                                 NaN           symbie   \n",
       "109                                    [spider, scrap]            scrap   \n",
       "\n",
       "    version   id  \n",
       "14    0.0.7   14  \n",
       "22   0.7.15   22  \n",
       "56    0.3.0   56  \n",
       "98    0.0.2   98  \n",
       "109   1.0.1  109  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifest_rows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtration\n",
    "\n",
    "Some fancy preprocessing using the Python multiprocessing package, here we add a new \"add_deps_resolved\" column to indicate that for a certain dependency all the packages that it in turn is dependent on have tags against them. Recall from the presentation that the representation of a single dependency requires collecting all its tags and the tags of its own dependents. We can take cases where we don't require all of the dependencies to have tags on them but for simplicity's sake we leave it so here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-07 23:19:30.431626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>allDependencies</th>\n",
       "      <th>dependencies</th>\n",
       "      <th>description</th>\n",
       "      <th>devDependencies</th>\n",
       "      <th>keywords</th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "      <th>id</th>\n",
       "      <th>all_deps_resolved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[async, connect-redis, express, nodeunit, redi...</td>\n",
       "      <td>[async, connect-redis, express, nodeunit, redi...</td>\n",
       "      <td>awesome framework</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>awesome</td>\n",
       "      <td>0.0.7</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[ejs, express, oauth, socket.io, underscore]</td>\n",
       "      <td>[ejs, express, oauth, socket.io, underscore]</td>\n",
       "      <td>IRC-like chatroom + Twitter live-stream</td>\n",
       "      <td>[]</td>\n",
       "      <td>[real-time, webapp, ExpressJS, socket.io, jQue...</td>\n",
       "      <td>socket-twitchat</td>\n",
       "      <td>0.7.15</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>[async, coffee-script, coloured-log, growl, op...</td>\n",
       "      <td>[async, coffee-script, coloured-log, growl, op...</td>\n",
       "      <td>Kontinuos Integrated Testing Koffee Applicatio...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[integration, testing, framework, expresso, ja...</td>\n",
       "      <td>kitkat</td>\n",
       "      <td>0.3.0</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[extx-reference-slot, extx-shotenjin, johnny-m...</td>\n",
       "      <td>[extx-reference-slot, extx-shotenjin, johnny-m...</td>\n",
       "      <td>Framework for web-sites running on the client</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>symbie</td>\n",
       "      <td>0.0.2</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>[bufferjs, html5, jsdom, sharedjs, underscore]</td>\n",
       "      <td>[bufferjs, html5, jsdom, sharedjs, underscore]</td>\n",
       "      <td>Easy way to write a spider.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[spider, scrap]</td>\n",
       "      <td>scrap</td>\n",
       "      <td>1.0.1</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       allDependencies  \\\n",
       "14   [async, connect-redis, express, nodeunit, redi...   \n",
       "22        [ejs, express, oauth, socket.io, underscore]   \n",
       "56   [async, coffee-script, coloured-log, growl, op...   \n",
       "98   [extx-reference-slot, extx-shotenjin, johnny-m...   \n",
       "109     [bufferjs, html5, jsdom, sharedjs, underscore]   \n",
       "\n",
       "                                          dependencies  \\\n",
       "14   [async, connect-redis, express, nodeunit, redi...   \n",
       "22        [ejs, express, oauth, socket.io, underscore]   \n",
       "56   [async, coffee-script, coloured-log, growl, op...   \n",
       "98   [extx-reference-slot, extx-shotenjin, johnny-m...   \n",
       "109     [bufferjs, html5, jsdom, sharedjs, underscore]   \n",
       "\n",
       "                                           description devDependencies  \\\n",
       "14                                   awesome framework              []   \n",
       "22             IRC-like chatroom + Twitter live-stream              []   \n",
       "56   Kontinuos Integrated Testing Koffee Applicatio...              []   \n",
       "98       Framework for web-sites running on the client              []   \n",
       "109                        Easy way to write a spider.              []   \n",
       "\n",
       "                                              keywords             name  \\\n",
       "14                                                 NaN          awesome   \n",
       "22   [real-time, webapp, ExpressJS, socket.io, jQue...  socket-twitchat   \n",
       "56   [integration, testing, framework, expresso, ja...           kitkat   \n",
       "98                                                 NaN           symbie   \n",
       "109                                    [spider, scrap]            scrap   \n",
       "\n",
       "    version   id  all_deps_resolved  \n",
       "14    0.0.7   14                  0  \n",
       "22   0.7.15   22                  0  \n",
       "56    0.3.0   56                  0  \n",
       "98    0.0.2   98                  0  \n",
       "109   1.0.1  109                  0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifest_rows['id'] = manifest_rows.index\n",
    "from pandas.util.testing import isiterable\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "i=1\n",
    "row_count = manifest_rows.shape[0]\n",
    "\n",
    "print(datetime.datetime.utcnow())\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "num_partitions = 8 #number of partitions to split dataframe\n",
    "num_cores = 8 #number of cores on your machine\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def check_deps_resolved(data):\n",
    "    data['all_deps_resolved'] = data['dependencies'].apply(add_dependencies_resolved_column)\n",
    "    return data\n",
    "\n",
    "def add_dependencies_resolved_column(dependencies):\n",
    "    dependencies = [dep.lower() for dep in dependencies]\n",
    "    packages_with_tags = packages_with_keywords.loc[packages_with_keywords['name'].isin(dependencies)]\n",
    "    \n",
    "    if len(packages_with_tags) == 0:\n",
    "        return 0\n",
    "    elif len(set(dependencies) - set(packages_with_tags['name'])) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "manifest_rows = parallelize_dataframe(manifest_rows, check_deps_resolved)\n",
    "manifest_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_rows = manifest_rows.loc[manifest_rows['all_deps_resolved'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27936\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>allDependencies</th>\n",
       "      <th>dependencies</th>\n",
       "      <th>description</th>\n",
       "      <th>devDependencies</th>\n",
       "      <th>keywords</th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "      <th>id</th>\n",
       "      <th>all_deps_resolved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>[async, chai, config, debug, piper, supercomfy...</td>\n",
       "      <td>[async, config, debug, piper, supercomfy, unde...</td>\n",
       "      <td>Simple mapping library for CouchDB</td>\n",
       "      <td>[chai]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flatpack</td>\n",
       "      <td>0.1.2</td>\n",
       "      <td>111</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>[backbone, markdown-js, mustache, promised-htt...</td>\n",
       "      <td>[backbone, markdown-js, mustache, promised-htt...</td>\n",
       "      <td>Teleport dashboard</td>\n",
       "      <td>[]</td>\n",
       "      <td>[teleport, dashboard, packages, dependencies]</td>\n",
       "      <td>teleport-dashboard</td>\n",
       "      <td>0.0.5</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>[async, coffee-script, colors, nock, pkginfo, ...</td>\n",
       "      <td>[async, colors, pkginfo, request, underscore]</td>\n",
       "      <td>Official node.js API client for scottyapp.com....</td>\n",
       "      <td>[coffee-script, nock, should, vows]</td>\n",
       "      <td>[scottyapp, api, rest, restful, client]</td>\n",
       "      <td>scottyapp-api-client</td>\n",
       "      <td>0.0.4</td>\n",
       "      <td>436</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>[async, colors, eventemitter2, request, util]</td>\n",
       "      <td>[async, colors, eventemitter2, request, util]</td>\n",
       "      <td>a decentralized, distributed, anonymous database</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hnet</td>\n",
       "      <td>0.0.1</td>\n",
       "      <td>492</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>[async, coffee-script, coffeekup, commander, c...</td>\n",
       "      <td>[async, commander, connect, express, lingo, mi...</td>\n",
       "      <td>Full Stack Web Framework for Node.js</td>\n",
       "      <td>[coffee-script, coffeekup, design.io, jade, ma...</td>\n",
       "      <td>[framework, rails, node]</td>\n",
       "      <td>coach</td>\n",
       "      <td>0.3.0</td>\n",
       "      <td>506</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       allDependencies  \\\n",
       "111  [async, chai, config, debug, piper, supercomfy...   \n",
       "149  [backbone, markdown-js, mustache, promised-htt...   \n",
       "436  [async, coffee-script, colors, nock, pkginfo, ...   \n",
       "492      [async, colors, eventemitter2, request, util]   \n",
       "506  [async, coffee-script, coffeekup, commander, c...   \n",
       "\n",
       "                                          dependencies  \\\n",
       "111  [async, config, debug, piper, supercomfy, unde...   \n",
       "149  [backbone, markdown-js, mustache, promised-htt...   \n",
       "436      [async, colors, pkginfo, request, underscore]   \n",
       "492      [async, colors, eventemitter2, request, util]   \n",
       "506  [async, commander, connect, express, lingo, mi...   \n",
       "\n",
       "                                           description  \\\n",
       "111                 Simple mapping library for CouchDB   \n",
       "149                                 Teleport dashboard   \n",
       "436  Official node.js API client for scottyapp.com....   \n",
       "492   a decentralized, distributed, anonymous database   \n",
       "506               Full Stack Web Framework for Node.js   \n",
       "\n",
       "                                       devDependencies  \\\n",
       "111                                             [chai]   \n",
       "149                                                 []   \n",
       "436                [coffee-script, nock, should, vows]   \n",
       "492                                                 []   \n",
       "506  [coffee-script, coffeekup, design.io, jade, ma...   \n",
       "\n",
       "                                          keywords                  name  \\\n",
       "111                                            NaN              flatpack   \n",
       "149  [teleport, dashboard, packages, dependencies]    teleport-dashboard   \n",
       "436        [scottyapp, api, rest, restful, client]  scottyapp-api-client   \n",
       "492                                            NaN                  hnet   \n",
       "506                       [framework, rails, node]                 coach   \n",
       "\n",
       "    version   id  all_deps_resolved  \n",
       "111   0.1.2  111                  1  \n",
       "149   0.0.5  149                  1  \n",
       "436   0.0.4  436                  1  \n",
       "492   0.0.1  492                  1  \n",
       "506   0.3.0  506                  1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(manifest_rows))\n",
    "manifest_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('manifest_dataset.json', 'w') as f:\n",
    "    f.write(manifest_rows.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary and package list\n",
    "\n",
    "Here we create a list of all the items(dependencies) present in our dataset. We also create a vocabulary based on the tags found against these packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of all packages list (no. of items): 29526\n"
     ]
    }
   ],
   "source": [
    "def clean_keywords(raw_keywords):\n",
    "    return set(keyword.strip().lower().replace(' ', '-') for keyword in raw_keywords if keyword.strip() is not '')\n",
    "\n",
    "\n",
    "def clean_dependencies(raw_keywords):\n",
    "    return set(keyword.lower() for keyword in raw_keywords)\n",
    "\n",
    "all_packages = set()\n",
    "all_keywords = set()\n",
    "list_of_manifest_list = []\n",
    "\n",
    "for idx, row in manifest_rows.iterrows():\n",
    "    all_packages = all_packages.union(set(dependency.lower() for dependency in row['dependencies']))\n",
    "    list_of_manifest_list.append(list(clean_dependencies(row['dependencies'])))\n",
    "\n",
    "vocabulary = set()\n",
    "\n",
    "package_tag_map = {}\n",
    "\n",
    "print(\"Length of all packages list (no. of items): {}\".format(len(all_packages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating direct tag mapping\n",
      "{'terminal', 'log', 'logging', 'xterm', 'cli', 'ansi', 'color', 'shell', 'console'}\n"
     ]
    }
   ],
   "source": [
    "# First take care of all the direct keywords\n",
    "print(\"Creating direct tag mapping\")\n",
    "i=0\n",
    "\n",
    "keywords_df = packages_with_keywords.loc[packages_with_keywords['name'].isin(all_packages), ['name','keywords']]\n",
    "# keywords_df.head()\n",
    "\n",
    "package_tag_map = {}\n",
    "for k,g in keywords_df.groupby(\"name\"):\n",
    "    package_tag_map[k] = clean_keywords(package_tag_map.get(k, set()).union(set(g[\"keywords\"].tolist()[0])))\n",
    "    vocabulary = vocabulary.union(package_tag_map[k])\n",
    "print(package_tag_map['cli-color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing first level of dependencies\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing first level of dependencies\")\n",
    "# Now the dependencies\n",
    "dependencies_df = packages_with_keywords.loc[packages_with_keywords['name'].isin(all_packages), ['name','dependencies']]\n",
    "\n",
    "package_dep_map = {}\n",
    "all_first_lv_deps = set()\n",
    "\n",
    "def clean_dependencies(raw_keywords):\n",
    "    return set(keyword.lower() for keyword in raw_keywords)\n",
    "\n",
    "for k,g in dependencies_df.groupby(\"name\"):\n",
    "    package_dep_map[k] = clean_dependencies(package_dep_map.get(k, set()).union(set(g[\"dependencies\"].tolist()[0])))\n",
    "    all_first_lv_deps = all_first_lv_deps.union(set(package_dep_map[k]))\n",
    "\n",
    "keywords_df_deps = packages_with_keywords.loc[packages_with_keywords['name'].isin(all_first_lv_deps), ['name','keywords']]\n",
    "\n",
    "extended_ptm = {}\n",
    "\n",
    "for k,g in keywords_df_deps.groupby(\"name\"):\n",
    "    extended_ptm[k] = clean_keywords(package_dep_map.get(k, set()).union(set(g[\"keywords\"].tolist()[0])))\n",
    "\n",
    "for package_name in package_tag_map.keys():\n",
    "    more_keywords = set()\n",
    "    for dependency in package_dep_map[package_name]:\n",
    "        more_keywords = more_keywords.union(set(extended_ptm.get(dependency, [])))\n",
    "    package_tag_map[package_name] = package_tag_map.get(package_name).union(more_keywords)\n",
    "    vocabulary = vocabulary.union(more_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now making the maps\n"
     ]
    }
   ],
   "source": [
    "print(\"Now making the maps\")\n",
    "pkg_idx_map = dict(zip(list(all_packages), range(len(all_packages))))\n",
    "idx_pkg_map = dict(zip(range(len(all_packages)), list(all_packages)))\n",
    "\n",
    "tag_idx_map = dict(zip(list(vocabulary), range(len(vocabulary))))\n",
    "idx_tag_map = dict(zip(range(len(vocabulary)), list(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting to 1: [14211, 5772, 1082, 26460, 7738, 24695, 32528, 18816]\n"
     ]
    }
   ],
   "source": [
    "# content matrix construction\n",
    "content_matrix = np.zeros([len(all_packages), len(vocabulary)])\n",
    "\n",
    "for idx, package in enumerate(all_packages):\n",
    "    this_package_tags = [tag_idx_map[tag] for tag in package_tag_map[package]]\n",
    "    if idx == 0:\n",
    "        print(\"Setting to 1: {}\".format(this_package_tags))\n",
    "    content_matrix[idx, this_package_tags] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33347\n",
      "29526\n",
      "27936\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary))\n",
    "print(len(all_packages))\n",
    "print(len(list_of_manifest_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the \"content matrix\" that maps all of our packages to the \"tag vocabulary\" that we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('content_matrix.npy', 'wb') as np_outfile:\n",
    "    np.save(np_outfile, content_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the \"rating matrix\". This contains our user-item i.e. stack-package pairs for the collaborative (probabilistic matrix factorization) part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating matrix construction\n",
    "\n",
    "with open('manifest_user_data.dat', 'w') as mud:\n",
    "    for manifest in list_of_manifest_list:\n",
    "        this_user_items = [pkg_idx_map[pkg] for pkg in manifest]\n",
    "        mud.write(\"{} {}\\n\".format(len(this_user_items), \" \".join(str(x) for x in this_user_items)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation to feed forward into our network, breakup into a training and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users=27936, num_items=29526\n",
      "packagedata-train-5-users.dat\n",
      "packagedata-train-5-items.dat\n",
      "packagedata-test-5-users.dat\n",
      "packagedata-test-5-items.dat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "pairs_train = []\n",
    "pairs_test = []\n",
    "num_train_per_user = 5\n",
    "user_id = 0\n",
    "np.random.seed(123)\n",
    "\n",
    "for line in open(\"manifest_user_data.dat\"):\n",
    "    arr = line.strip().split()\n",
    "    arr = np.asarray([int(x) for x in arr[1:]])\n",
    "    n = len(arr)\n",
    "    idx = np.random.permutation(n)\n",
    "    # assert(n > num_train_per_user)\n",
    "    for i in range(min(num_train_per_user, n)):\n",
    "        # Add num_train_per_user or all of the user's items to the training data.\n",
    "        pairs_train.append((user_id, arr[idx[i]]))\n",
    "    # if we have more items than we need for training, append to testing.\n",
    "    if n > num_train_per_user:\n",
    "        for i in range(num_train_per_user, n):\n",
    "            pairs_test.append((user_id, arr[idx[i]]))\n",
    "    user_id += 1\n",
    "num_users = user_id\n",
    "pairs_train = np.asarray(pairs_train)\n",
    "pairs_test = np.asarray(pairs_test)\n",
    "num_items = np.maximum(np.max(pairs_train[:, 1]), np.max(pairs_test[:, 1]))+1\n",
    "print(\"num_users=%d, num_items=%d\" % (num_users, num_items))\n",
    "\n",
    "# Row - users, column - items\n",
    "with open(\"packagedata-train-\"+str(num_train_per_user)+\"-users.dat\", \"w\") as fid:\n",
    "    print(fid.name)\n",
    "    for user_id in range(num_users):\n",
    "        # Collect all items of this user.\n",
    "        this_user_items = pairs_train[pairs_train[:, 0]==user_id, 1]\n",
    "        # Convert to a space separated string of integers\n",
    "        items_str = \" \".join(str(x) for x in this_user_items)\n",
    "        fid.write(\"%d %s\\n\" % (len(this_user_items), items_str))\n",
    "\n",
    "# Row - items, column - users\n",
    "with open(\"packagedata-train-\"+str(num_train_per_user)+\"-items.dat\", \"w\") as fid:\n",
    "    print(fid.name)\n",
    "    for item_id in range(num_items):\n",
    "        this_item_users = pairs_train[pairs_train[:, 1]==item_id, 0]\n",
    "        users_str = \" \".join(str(x) for x in this_item_users)\n",
    "        fid.write(\"%d %s\\n\" % (len(this_item_users), users_str))\n",
    "\n",
    "with open(\"packagedata-test-\"+str(num_train_per_user)+\"-users.dat\", \"w\") as fid:\n",
    "    print(fid.name)\n",
    "    for user_id in range(num_users):\n",
    "        this_user_items = pairs_test[pairs_test[:, 0]==user_id, 1]\n",
    "        items_str = \" \".join(str(x) for x in this_user_items)\n",
    "        fid.write(\"%d %s\\n\" % (len(this_user_items), items_str))\n",
    "\n",
    "with open(\"packagedata-test-\"+str(num_train_per_user)+\"-items.dat\", \"w\") as fid:\n",
    "    print(fid.name)\n",
    "    for item_id in range(num_items):\n",
    "        this_item_users = pairs_test[pairs_test[:, 1]==item_id, 0]\n",
    "        users_str = \" \".join(str(x) for x in this_item_users)\n",
    "        fid.write(\"%d %s\\n\" % (len(this_item_users), users_str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining the network in stacked Denoising autoencoder fasion\n",
    "\n",
    "- Pretrain the hidden layers\n",
    "- Pretrain the latent layer\n",
    "- Pretrain the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-08 07:50:51,156 [3357] INFO     root: loading data\n",
      "2019-08-08 07:50:51,156 [INFO]: loading data\n",
      "2019-08-08 07:50:51,156 [INFO]: loading data\n",
      "2019-08-08 07:50:51,156 [INFO]: loading data\n",
      "2019-08-08 07:50:51,156 [INFO]: loading data\n",
      "2019-08-08 07:50:51,156 [INFO]: loading data\n",
      "2019-08-08 07:50:51,156 [INFO]: loading data\n",
      "2019-08-08 07:50:51,156 [INFO]: loading data\n",
      "2019-08-08 07:50:51,156 [INFO]: loading data\n",
      "2019-08-08 07:50:51,156 [INFO]: loading data\n",
      "2019-08-08 07:50:51,156 [INFO]: loading data\n",
      "2019-08-08 07:50:51,156 [INFO]: loading data\n",
      "2019-08-08 07:50:51,156 [INFO]: loading data\n",
      "2019-08-08 07:51:15,219 [3357] INFO     root: initializing sdae model\n",
      "2019-08-08 07:51:15,219 [INFO]: initializing sdae model\n",
      "2019-08-08 07:51:15,219 [INFO]: initializing sdae model\n",
      "2019-08-08 07:51:15,219 [INFO]: initializing sdae model\n",
      "2019-08-08 07:51:15,219 [INFO]: initializing sdae model\n",
      "2019-08-08 07:51:15,219 [INFO]: initializing sdae model\n",
      "2019-08-08 07:51:15,219 [INFO]: initializing sdae model\n",
      "2019-08-08 07:51:15,219 [INFO]: initializing sdae model\n",
      "2019-08-08 07:51:15,219 [INFO]: initializing sdae model\n",
      "2019-08-08 07:51:15,219 [INFO]: initializing sdae model\n",
      "2019-08-08 07:51:15,219 [INFO]: initializing sdae model\n",
      "2019-08-08 07:51:15,219 [INFO]: initializing sdae model\n",
      "2019-08-08 07:51:15,219 [INFO]: initializing sdae model\n",
      "2019-08-08 07:51:15,601 [3357] INFO     root: fitting data starts...\n",
      "2019-08-08 07:51:15,601 [INFO]: fitting data starts...\n",
      "2019-08-08 07:51:15,601 [INFO]: fitting data starts...\n",
      "2019-08-08 07:51:15,601 [INFO]: fitting data starts...\n",
      "2019-08-08 07:51:15,601 [INFO]: fitting data starts...\n",
      "2019-08-08 07:51:15,601 [INFO]: fitting data starts...\n",
      "2019-08-08 07:51:15,601 [INFO]: fitting data starts...\n",
      "2019-08-08 07:51:15,601 [INFO]: fitting data starts...\n",
      "2019-08-08 07:51:15,601 [INFO]: fitting data starts...\n",
      "2019-08-08 07:51:15,601 [INFO]: fitting data starts...\n",
      "2019-08-08 07:51:15,601 [INFO]: fitting data starts...\n",
      "2019-08-08 07:51:15,601 [INFO]: fitting data starts...\n",
      "2019-08-08 07:51:15,601 [INFO]: fitting data starts...\n",
      "2019-08-08 07:51:15,614 [3357] INFO     root: Layer 1\n",
      "2019-08-08 07:51:15,614 [INFO]: Layer 1\n",
      "2019-08-08 07:51:15,614 [INFO]: Layer 1\n",
      "2019-08-08 07:51:15,614 [INFO]: Layer 1\n",
      "2019-08-08 07:51:15,614 [INFO]: Layer 1\n",
      "2019-08-08 07:51:15,614 [INFO]: Layer 1\n",
      "2019-08-08 07:51:15,614 [INFO]: Layer 1\n",
      "2019-08-08 07:51:15,614 [INFO]: Layer 1\n",
      "2019-08-08 07:51:15,614 [INFO]: Layer 1\n",
      "2019-08-08 07:51:15,614 [INFO]: Layer 1\n",
      "2019-08-08 07:51:15,614 [INFO]: Layer 1\n",
      "2019-08-08 07:51:15,614 [INFO]: Layer 1\n",
      "2019-08-08 07:51:15,614 [INFO]: Layer 1\n",
      "2019-08-08 07:53:10,481 [3357] INFO     root: epoch 0: batch loss = 1249.5517578125\n",
      "2019-08-08 07:53:10,481 [INFO]: epoch 0: batch loss = 1249.5517578125\n",
      "2019-08-08 07:53:10,481 [INFO]: epoch 0: batch loss = 1249.5517578125\n",
      "2019-08-08 07:53:10,481 [INFO]: epoch 0: batch loss = 1249.5517578125\n",
      "2019-08-08 07:53:10,481 [INFO]: epoch 0: batch loss = 1249.5517578125\n",
      "2019-08-08 07:53:10,481 [INFO]: epoch 0: batch loss = 1249.5517578125\n",
      "2019-08-08 07:53:10,481 [INFO]: epoch 0: batch loss = 1249.5517578125\n",
      "2019-08-08 07:53:10,481 [INFO]: epoch 0: batch loss = 1249.5517578125\n",
      "2019-08-08 07:53:10,481 [INFO]: epoch 0: batch loss = 1249.5517578125\n",
      "2019-08-08 07:53:10,481 [INFO]: epoch 0: batch loss = 1249.5517578125\n",
      "2019-08-08 07:53:10,481 [INFO]: epoch 0: batch loss = 1249.5517578125\n",
      "2019-08-08 07:53:10,481 [INFO]: epoch 0: batch loss = 1249.5517578125\n",
      "2019-08-08 07:53:10,481 [INFO]: epoch 0: batch loss = 1249.5517578125\n",
      "2019-08-08 07:53:20,451 [3357] INFO     root: Layer 2\n",
      "2019-08-08 07:53:20,451 [INFO]: Layer 2\n",
      "2019-08-08 07:53:20,451 [INFO]: Layer 2\n",
      "2019-08-08 07:53:20,451 [INFO]: Layer 2\n",
      "2019-08-08 07:53:20,451 [INFO]: Layer 2\n",
      "2019-08-08 07:53:20,451 [INFO]: Layer 2\n",
      "2019-08-08 07:53:20,451 [INFO]: Layer 2\n",
      "2019-08-08 07:53:20,451 [INFO]: Layer 2\n",
      "2019-08-08 07:53:20,451 [INFO]: Layer 2\n",
      "2019-08-08 07:53:20,451 [INFO]: Layer 2\n",
      "2019-08-08 07:53:20,451 [INFO]: Layer 2\n",
      "2019-08-08 07:53:20,451 [INFO]: Layer 2\n",
      "2019-08-08 07:53:20,451 [INFO]: Layer 2\n",
      "2019-08-08 07:53:22,455 [3357] INFO     root: epoch 0: batch loss = 130.42559814453125\n",
      "2019-08-08 07:53:22,455 [INFO]: epoch 0: batch loss = 130.42559814453125\n",
      "2019-08-08 07:53:22,455 [INFO]: epoch 0: batch loss = 130.42559814453125\n",
      "2019-08-08 07:53:22,455 [INFO]: epoch 0: batch loss = 130.42559814453125\n",
      "2019-08-08 07:53:22,455 [INFO]: epoch 0: batch loss = 130.42559814453125\n",
      "2019-08-08 07:53:22,455 [INFO]: epoch 0: batch loss = 130.42559814453125\n",
      "2019-08-08 07:53:22,455 [INFO]: epoch 0: batch loss = 130.42559814453125\n",
      "2019-08-08 07:53:22,455 [INFO]: epoch 0: batch loss = 130.42559814453125\n",
      "2019-08-08 07:53:22,455 [INFO]: epoch 0: batch loss = 130.42559814453125\n",
      "2019-08-08 07:53:22,455 [INFO]: epoch 0: batch loss = 130.42559814453125\n",
      "2019-08-08 07:53:22,455 [INFO]: epoch 0: batch loss = 130.42559814453125\n",
      "2019-08-08 07:53:22,455 [INFO]: epoch 0: batch loss = 130.42559814453125\n",
      "2019-08-08 07:53:22,455 [INFO]: epoch 0: batch loss = 130.42559814453125\n",
      "2019-08-08 07:53:23,905 [3357] INFO     root: epoch 0: batch loss = 25.374744415283203, gen_loss=23.435379028320312, latent_loss=1.9393656253814697\n",
      "2019-08-08 07:53:23,905 [INFO]: epoch 0: batch loss = 25.374744415283203, gen_loss=23.435379028320312, latent_loss=1.9393656253814697\n",
      "2019-08-08 07:53:23,905 [INFO]: epoch 0: batch loss = 25.374744415283203, gen_loss=23.435379028320312, latent_loss=1.9393656253814697\n",
      "2019-08-08 07:53:23,905 [INFO]: epoch 0: batch loss = 25.374744415283203, gen_loss=23.435379028320312, latent_loss=1.9393656253814697\n",
      "2019-08-08 07:53:23,905 [INFO]: epoch 0: batch loss = 25.374744415283203, gen_loss=23.435379028320312, latent_loss=1.9393656253814697\n",
      "2019-08-08 07:53:23,905 [INFO]: epoch 0: batch loss = 25.374744415283203, gen_loss=23.435379028320312, latent_loss=1.9393656253814697\n",
      "2019-08-08 07:53:23,905 [INFO]: epoch 0: batch loss = 25.374744415283203, gen_loss=23.435379028320312, latent_loss=1.9393656253814697\n",
      "2019-08-08 07:53:23,905 [INFO]: epoch 0: batch loss = 25.374744415283203, gen_loss=23.435379028320312, latent_loss=1.9393656253814697\n",
      "2019-08-08 07:53:23,905 [INFO]: epoch 0: batch loss = 25.374744415283203, gen_loss=23.435379028320312, latent_loss=1.9393656253814697\n",
      "2019-08-08 07:53:23,905 [INFO]: epoch 0: batch loss = 25.374744415283203, gen_loss=23.435379028320312, latent_loss=1.9393656253814697\n",
      "2019-08-08 07:53:23,905 [INFO]: epoch 0: batch loss = 25.374744415283203, gen_loss=23.435379028320312, latent_loss=1.9393656253814697\n",
      "2019-08-08 07:53:23,905 [INFO]: epoch 0: batch loss = 25.374744415283203, gen_loss=23.435379028320312, latent_loss=1.9393656253814697\n",
      "2019-08-08 07:53:23,905 [INFO]: epoch 0: batch loss = 25.374744415283203, gen_loss=23.435379028320312, latent_loss=1.9393656253814697\n",
      "2019-08-08 07:54:41,693 [3357] INFO     root: epoch 0: batch loss = 211.0083770751953, gen_loss=177.51190185546875, latent_loss=33.49647903442383, valid_loss=178.22448120117187\n",
      "2019-08-08 07:54:41,693 [INFO]: epoch 0: batch loss = 211.0083770751953, gen_loss=177.51190185546875, latent_loss=33.49647903442383, valid_loss=178.22448120117187\n",
      "2019-08-08 07:54:41,693 [INFO]: epoch 0: batch loss = 211.0083770751953, gen_loss=177.51190185546875, latent_loss=33.49647903442383, valid_loss=178.22448120117187\n",
      "2019-08-08 07:54:41,693 [INFO]: epoch 0: batch loss = 211.0083770751953, gen_loss=177.51190185546875, latent_loss=33.49647903442383, valid_loss=178.22448120117187\n",
      "2019-08-08 07:54:41,693 [INFO]: epoch 0: batch loss = 211.0083770751953, gen_loss=177.51190185546875, latent_loss=33.49647903442383, valid_loss=178.22448120117187\n",
      "2019-08-08 07:54:41,693 [INFO]: epoch 0: batch loss = 211.0083770751953, gen_loss=177.51190185546875, latent_loss=33.49647903442383, valid_loss=178.22448120117187\n",
      "2019-08-08 07:54:41,693 [INFO]: epoch 0: batch loss = 211.0083770751953, gen_loss=177.51190185546875, latent_loss=33.49647903442383, valid_loss=178.22448120117187\n",
      "2019-08-08 07:54:41,693 [INFO]: epoch 0: batch loss = 211.0083770751953, gen_loss=177.51190185546875, latent_loss=33.49647903442383, valid_loss=178.22448120117187\n",
      "2019-08-08 07:54:41,693 [INFO]: epoch 0: batch loss = 211.0083770751953, gen_loss=177.51190185546875, latent_loss=33.49647903442383, valid_loss=178.22448120117187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-08 07:54:41,693 [INFO]: epoch 0: batch loss = 211.0083770751953, gen_loss=177.51190185546875, latent_loss=33.49647903442383, valid_loss=178.22448120117187\n",
      "2019-08-08 07:54:41,693 [INFO]: epoch 0: batch loss = 211.0083770751953, gen_loss=177.51190185546875, latent_loss=33.49647903442383, valid_loss=178.22448120117187\n",
      "2019-08-08 07:54:41,693 [INFO]: epoch 0: batch loss = 211.0083770751953, gen_loss=177.51190185546875, latent_loss=33.49647903442383, valid_loss=178.22448120117187\n",
      "2019-08-08 07:54:41,693 [INFO]: epoch 0: batch loss = 211.0083770751953, gen_loss=177.51190185546875, latent_loss=33.49647903442383, valid_loss=178.22448120117187\n",
      "2019-08-08 07:54:42,426 [3357] INFO     root: Weights saved at model/pretrain\n",
      "2019-08-08 07:54:42,426 [INFO]: Weights saved at model/pretrain\n",
      "2019-08-08 07:54:42,426 [INFO]: Weights saved at model/pretrain\n",
      "2019-08-08 07:54:42,426 [INFO]: Weights saved at model/pretrain\n",
      "2019-08-08 07:54:42,426 [INFO]: Weights saved at model/pretrain\n",
      "2019-08-08 07:54:42,426 [INFO]: Weights saved at model/pretrain\n",
      "2019-08-08 07:54:42,426 [INFO]: Weights saved at model/pretrain\n",
      "2019-08-08 07:54:42,426 [INFO]: Weights saved at model/pretrain\n",
      "2019-08-08 07:54:42,426 [INFO]: Weights saved at model/pretrain\n",
      "2019-08-08 07:54:42,426 [INFO]: Weights saved at model/pretrain\n",
      "2019-08-08 07:54:42,426 [INFO]: Weights saved at model/pretrain\n",
      "2019-08-08 07:54:42,426 [INFO]: Weights saved at model/pretrain\n",
      "2019-08-08 07:54:42,426 [INFO]: Weights saved at model/pretrain\n"
     ]
    }
   ],
   "source": [
    "# %load test_vae_package_data.py\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from aelib.vae import VariationalAutoEncoder\n",
    "from aelib.utils import *\n",
    "from scipy import sparse\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "    # sess = tf.Session(config=tf.ConfigProto(log_device_placement=True, gpu_options=gpu_options))\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "    np.random.seed(0)\n",
    "    tf.set_random_seed(0)\n",
    "    init_logging(\"vae_pretrain_package_data.log\")\n",
    "\n",
    "    logging.info('loading data')\n",
    "\n",
    "    content_matrix_file = open('content_matrix.npy', 'rb')\n",
    "    data = np.load(content_matrix_file)\n",
    "    content_matrix_file.close()\n",
    "#     csr_sparse = sparse.load_npz('content_matrix.npz')\n",
    "#     data = csr_sparse.toarray()\n",
    "\n",
    "    idx = np.random.rand(data.shape[0]) < 0.8\n",
    "    train_X = data[idx]\n",
    "    test_X = data[~idx]\n",
    "    logging.info('initializing sdae model')\n",
    "    model = VariationalAutoEncoder(input_dim=37617, dims=[200, 100], z_dim=50,\n",
    "                                   activations=['sigmoid', 'sigmoid'], epoch=[1, 1],\n",
    "                                   noise='mask-0.3', loss='cross-entropy', lr=0.01, batch_size=1024, print_step=1)\n",
    "    logging.info('fitting data starts...')\n",
    "    model.fit(train_X, test_X)\n",
    "# feat = model.transform(data)\n",
    "# scipy.io.savemat('feat-dae.mat',{'feat': feat})\n",
    "# np.savez(\"sdae-weights.npz\", en_weights=model.weights, en_biases=model.biases,\n",
    "# \tde_weights=model.de_weights, de_biases=model.de_biases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We train the variational autoencoder and the PMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avgupta/.pyenv/versions/anaconda3-5.2.0/envs/npm-insights-attempt-2/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/avgupta/.pyenv/versions/anaconda3-5.2.0/envs/npm-insights-attempt-2/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only initializing x_recon\n",
      "confirmed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-08 11:58:43,836 [8153] INFO     root: Loading weights from model/pretrain\n",
      "2019-08-08 11:58:43,836 [INFO]: Loading weights from model/pretrain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/pretrain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-08 11:58:43,837 [8153] INFO     tensorflow: Restoring parameters from model/pretrain\n",
      "2019-08-08 11:58:43,837 [INFO]: Restoring parameters from model/pretrain\n",
      "2019-08-08 12:00:55,309 [8153] INFO     root: [#epoch=000000], loss=1733259.76306, neg_likelihood=41497.92745, gen_loss=114.59472\n",
      "2019-08-08 12:00:55,309 [INFO]: [#epoch=000000], loss=1733259.76306, neg_likelihood=41497.92745, gen_loss=114.59472\n",
      "2019-08-08 12:00:55,659 [8153] INFO     root: Weights saved at /Users/avgupta/weights/train/cvae-packagedata\n",
      "2019-08-08 12:00:55,659 [INFO]: Weights saved at /Users/avgupta/weights/train/cvae-packagedata\n",
      "2019-08-08 12:00:55,792 [8153] INFO     root: Matrices saved at /Users/avgupta/weights/train/pmf-packagedata\n",
      "2019-08-08 12:00:55,792 [INFO]: Matrices saved at /Users/avgupta/weights/train/pmf-packagedata\n"
     ]
    }
   ],
   "source": [
    "# %load test_cvae-packagedata5.py\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from aelib.cvae import *\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "from scipy.sparse import load_npz\n",
    "from aelib.utils import *\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "init_logging(\"cvae-packagedata-5.log\")\n",
    "\n",
    "def load_cvae_data():\n",
    "  data = {}\n",
    "  data_dir = \"./\"\n",
    "  # data_dir = \"retrain/\"\n",
    "  # variables = scipy.io.loadmat(data_dir + \"mult_nor.mat\")\n",
    "  # data[\"content\"] = variables['X']\n",
    "\n",
    "  d = np.load('content_matrix.npy')\n",
    "\n",
    "  # d = d.T\n",
    "  data[\"content\"] = d\n",
    "\n",
    "  data[\"train_users\"] = load_rating(data_dir + \"packagedata-train-5-users.dat\")\n",
    "  data[\"train_items\"] = load_rating(data_dir + \"packagedata-train-5-items.dat\")\n",
    "  data[\"test_users\"] = load_rating(data_dir + \"packagedata-test-5-users.dat\")\n",
    "  data[\"test_items\"] = load_rating(data_dir + \"packagedata-test-5-items.dat\")\n",
    "\n",
    "  return data\n",
    "\n",
    "def load_rating(path):\n",
    "  arr = []\n",
    "  for line in open(path):\n",
    "    a = line.strip().split()\n",
    "    if a[0]==0:\n",
    "      l = []\n",
    "    else:\n",
    "      l = [int(x) for x in a[1:]]\n",
    "    arr.append(l)\n",
    "  return arr\n",
    "\n",
    "params = Params()\n",
    "params.lambda_u = 0.1\n",
    "params.lambda_v = 10\n",
    "params.lambda_r = 1\n",
    "params.a = 1\n",
    "params.b = 0.01\n",
    "params.M = 300\n",
    "params.n_epochs = 1\n",
    "params.max_iter = 1\n",
    "\n",
    "data = load_cvae_data()\n",
    "num_factors = 50\n",
    "model = CVAE(num_users=27936, num_items=29526, num_factors=num_factors, params=params,\n",
    "    input_dim=33347, dims=[200, 100], n_z=num_factors, activations=['sigmoid', 'sigmoid'],\n",
    "    loss_type='cross-entropy', lr=0.001, random_seed=0, print_step=10, verbose=False)\n",
    "model.load_model(weight_path=\"model/pretrain\")\n",
    "# model.load_model(weight_path=\"/Users/avgupta/s3/avgupta-stack-analysis-dev/weights/pretrain/pretrain\")\n",
    "\n",
    "model.run(data[\"train_users\"], data[\"train_items\"], data[\"test_users\"], data[\"test_items\"],\n",
    "   data[\"content\"], params)\n",
    "model.save_model(weight_path=\"weights/train/cvae-packagedata\", pmf_path=\"weights/train/pmf-packagedata\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
