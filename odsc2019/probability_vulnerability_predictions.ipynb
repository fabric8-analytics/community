{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Necessary Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/redanalyze/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import text_normalizer as tn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import dill\n",
    "import gc\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Events Data from GitHub\n",
    "\n",
    "Focus is on\n",
    "- Issues\n",
    "- Pull Requests\n",
    "- Commits (future scope)\n",
    "\n",
    "Data is based on feeds from popular golang dependencies\\repos which had vulnerabilities in the past to get both positive and negative data points in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 152151 entries, 0 to 152150\n",
      "Data columns (total 2 columns):\n",
      "description    152151 non-null object\n",
      "label          152151 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/gokube_phase1_jun19/GH_complete_labeled_issues_prs.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['description'].values\n",
    "labels = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing: starting\n",
      "ThreadPoolExecutor-0_0: working on doc num: 0\n",
      "ThreadPoolExecutor-0_2: working on doc num: 5000\n",
      "ThreadPoolExecutor-0_13: working on doc num: 10000\n",
      "ThreadPoolExecutor-0_14: working on doc num: 15000\n",
      "ThreadPoolExecutor-0_11: working on doc num: 20000\n",
      "ThreadPoolExecutor-0_0: working on doc num: 25000\n",
      "ThreadPoolExecutor-0_3: working on doc num: 30000\n",
      "ThreadPoolExecutor-0_3: working on doc num: 35000\n",
      "ThreadPoolExecutor-0_20: working on doc num: 40000\n",
      "ThreadPoolExecutor-0_28: working on doc num: 45000\n",
      "ThreadPoolExecutor-0_29: working on doc num: 50000\n",
      "ThreadPoolExecutor-0_23: working on doc num: 55000\n",
      "ThreadPoolExecutor-0_24: working on doc num: 60000\n",
      "ThreadPoolExecutor-0_5: working on doc num: 65000\n",
      "ThreadPoolExecutor-0_27: working on doc num: 70000\n",
      "ThreadPoolExecutor-0_26: working on doc num: 75000\n",
      "ThreadPoolExecutor-0_28: working on doc num: 80000\n",
      "ThreadPoolExecutor-0_5: working on doc num: 85000\n",
      "ThreadPoolExecutor-0_7: working on doc num: 90000\n",
      "ThreadPoolExecutor-0_24: working on doc num: 95000\n",
      "ThreadPoolExecutor-0_10: working on doc num: 100000\n",
      "ThreadPoolExecutor-0_5: working on doc num: 105000\n",
      "ThreadPoolExecutor-0_29: working on doc num: 110000\n",
      "ThreadPoolExecutor-0_0: working on doc num: 115000\n",
      "ThreadPoolExecutor-0_17: working on doc num: 120000\n",
      "ThreadPoolExecutor-0_15: working on doc num: 125000\n",
      "ThreadPoolExecutor-0_0: working on doc num: 130000\n",
      "ThreadPoolExecutor-0_4: working on doc num: 135000\n",
      "ThreadPoolExecutor-0_17: working on doc num: 140000\n",
      "ThreadPoolExecutor-0_20: working on doc num: 145000\n",
      "ThreadPoolExecutor-0_12: working on doc num: 150000\n",
      "ThreadPoolExecutor-0_9: working on doc num: 152150\n",
      "152151\n",
      "CPU times: user 16min 25s, sys: 37.6 s, total: 17min 2s\n",
      "Wall time: 16min\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "norm_docs = tn.pre_process_documents_parallel(documents=docs)\n",
    "print(len(norm_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./data/gokube_phase1_jun19/gh_preprocessed_descriptions.pkl', 'wb') as f:\n",
    "#    dill.dump(norm_docs, f)\n",
    "#    \n",
    "#with open('./data/gokube_phase1_jun19/gh_labels.pkl', 'wb') as f:\n",
    "#    dill.dump(labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/gokube_phase1_jun19/gh_preprocessed_descriptions.pkl', 'rb') as f:\n",
    "    norm_docs = dill.load(f)\n",
    "    \n",
    "#with open('./data/gokube_phase1_jun19/gh_labels.pkl', 'rb') as f:\n",
    "#    labels = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23243, (23243,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_data = []\n",
    "positive_labels = []\n",
    "for doc, label in zip(norm_docs, labels):\n",
    "    if label != 0:\n",
    "        positive_data.append(doc)\n",
    "        positive_labels.append(label)\n",
    "        \n",
    "norm_docs = positive_data\n",
    "labels = np.array(positive_labels) - 1\n",
    "len(norm_docs), labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 671, 0: 22572})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on 75:25 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17432, 5811)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(norm_docs, labels, \n",
    "                                                    test_size=0.25, random_state=SEED)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some config values \n",
    "EMBED_SIZE = 300 # how big is each word vector\n",
    "MAX_FEATURES = 800000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_LEN = 1000 # max number of words in a doc to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tokenizer for word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "CVE_WORD2IDX_MAP_FILE = 'models/v3-jun19/embeddings/cve_tokenizer_word2idx.pkl'\n",
    "\n",
    "if not os.path.isfile(CVE_WORD2IDX_MAP_FILE):\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(oov_token='<UNK>', num_words=MAX_FEATURES+1)\n",
    "    tokenizer.fit_on_texts(list(X_train))\n",
    "    tokenizer.word_index['<PAD>'] = 0\n",
    "    with open(CVE_WORD2IDX_MAP_FILE, 'wb') as f:\n",
    "        dill.dump(tokenizer.word_index, f)\n",
    "else:\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(oov_token='<UNK>', num_words=MAX_FEATURES+1)\n",
    "    with open(CVE_WORD2IDX_MAP_FILE, 'rb') as f:\n",
    "        word2idx = dill.load(f)\n",
    "    tokenizer.word_index = word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291899"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_FEATURES = len(tokenizer.word_index)\n",
    "MAX_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pad sentences to sequence length of 1000 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the sentences\n",
    "train_X = tokenizer.texts_to_sequences(X_train)\n",
    "test_X = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pad the sentences \n",
    "train_X = keras.preprocessing.sequence.pad_sequences(train_X, maxlen=MAX_LEN)\n",
    "test_X = keras.preprocessing.sequence.pad_sequences(test_X, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17389, 1000), (17389,), (5798, 1000), (5798,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_lengths = np.array([len(np.nonzero(item)[0]) for item in train_X])\n",
    "train_X_idx = np.argwhere(train_X_lengths >= 5).ravel()\n",
    "train_X = train_X[train_X_idx]\n",
    "train_y = y_train[train_X_idx]\n",
    "\n",
    "test_X_lengths = np.array([len(np.nonzero(item)[0]) for item in test_X])\n",
    "test_X_idx = np.argwhere(test_X_lengths >= 5).ravel()\n",
    "test_X = test_X[test_X_idx]\n",
    "test_y = y_test[test_X_idx]\n",
    "train_X.shape, train_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-trained Embeddings\n",
    "\n",
    "We have experimented with the following embeddings (pre-trained models)\n",
    "\n",
    "- FastText\n",
    "- ParaGram\n",
    "- GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_embeddings(word_to_index, max_features, embedding_size, embedding_file_path):    \n",
    "    \n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*row.split(\" \")) \n",
    "                                for row in open(embedding_file_path, encoding=\"utf8\", errors='ignore') \n",
    "                                    if len(row)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    nb_words = min(max_features, len(word_to_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_size))\n",
    "    \n",
    "    for word, idx in word_to_index.items():\n",
    "        if idx >= max_features: \n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291899, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FASTTEXT_INIT_EMBEDDINGS_FILE = './models/v3-jun19/embeddings/cve_model_fasttext_init_embeddings.pkl'\n",
    "\n",
    "if not os.path.isfile(FASTTEXT_INIT_EMBEDDINGS_FILE):\n",
    "    FASTTEXT_EMBEDDINGS_PATH = './embeddings/fasttext/crawl-300d-2M.vec'\n",
    "    ft_embeddings = load_pretrained_embeddings(word_to_index=word2idx, max_features=MAX_FEATURES, \n",
    "                                               embedding_size=EMBED_SIZE, \n",
    "                                               embedding_file_path=FASTTEXT_EMBEDDINGS_PATH)\n",
    "    with open(FASTTEXT_INIT_EMBEDDINGS_FILE, 'wb') as f:\n",
    "        dill.dump(ft_embeddings, f)\n",
    "else:\n",
    "    with open(FASTTEXT_INIT_EMBEDDINGS_FILE, 'rb') as f:\n",
    "        ft_embeddings = dill.load(f)\n",
    "        \n",
    "ft_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291899, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAGRAM_INIT_EMBEDDINGS_FILE = './models/v3-jun19/embeddings/cve_model_paragram_init_embeddings.pkl'\n",
    "\n",
    "if not os.path.isfile(PARAGRAM_INIT_EMBEDDINGS_FILE):\n",
    "    PARAGRAM_EMBEDDINGS_PATH = './embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    pg_embeddings = load_pretrained_embeddings(word_to_index=word2idx, max_features=MAX_FEATURES, \n",
    "                                               embedding_size=EMBED_SIZE, \n",
    "                                               embedding_file_path=PARAGRAM_EMBEDDINGS_PATH)\n",
    "    with open(PARAGRAM_INIT_EMBEDDINGS_FILE, 'wb') as f:\n",
    "        dill.dump(pg_embeddings, f)\n",
    "else:\n",
    "    with open(PARAGRAM_INIT_EMBEDDINGS_FILE, 'rb') as f:\n",
    "        pg_embeddings = dill.load(f)\n",
    "        \n",
    "pg_embeddings.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average pre-trained embeddings for vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291899, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_pretrained_embeddings = np.mean([ft_embeddings, pg_embeddings], axis = 0)\n",
    "avg_pretrained_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model Architecture\n",
    "\n",
    "### Attention Layer\n",
    "\n",
    "Attention Layer focuses on attending to the most important words. We sent all the states from our GRU model into the attention model.\n",
    "\n",
    "![](https://i.imgur.com/vbGl6Vl.png)\n",
    "\n",
    "The attention layer produces a context vector\n",
    "\n",
    "![](https://i.imgur.com/nZ71MVd.png)\n",
    "\n",
    "![](https://i.imgur.com/00KyS2e.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.supports_masking = True\n",
    "        self.init = keras.initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = keras.regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = keras.regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = keras.constraints.get(W_constraint)\n",
    "        self.b_constraint = keras.constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "        \n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), \n",
    "                              K.reshape(self.W, (features_dim, 1))),\n",
    "                        (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        \n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'step_dim': self.step_dim}\n",
    "        base_config = super(AttentionLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-directional GRUs\n",
    "\n",
    "![](https://i.imgur.com/PuTHi2C.png)\n",
    "\n",
    "![](https://i.imgur.com/ewTg3gB.png)\n",
    "\n",
    "![](https://i.imgur.com/oaBYGeu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "def build_gru_model(embedding_matrix, embedding_size, max_len, max_features, gru_units=32):\n",
    "    \n",
    "    inp = keras.layers.Input(shape=(max_len,))\n",
    "    x = keras.layers.Embedding(max_features, embedding_size, \n",
    "                                  weights=[embedding_matrix], trainable=True)(inp)\n",
    "    x = keras.layers.Bidirectional(keras.layers.CuDNNGRU(gru_units*2, return_sequences=True))(x)\n",
    "    x = keras.layers.Bidirectional(keras.layers.CuDNNGRU(gru_units, return_sequences=True))(x)\n",
    "    x = AttentionLayer(max_len)(x)\n",
    "    x = keras.layers.Dense(gru_units*2, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(rate=0.2)(x)\n",
    "    x = keras.layers.Dense(gru_units, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(rate=0.2)(x)\n",
    "\n",
    "    outp = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    # initialize the model\n",
    "    model = keras.models.Model(inputs=inp, outputs=outp)\n",
    "\n",
    "    # make the model parallel\n",
    "    #model = multi_gpu_model(model, gpus=2)\n",
    "       \n",
    "    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         87569700  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1000, 128)         140544    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 1000, 64)          31104     \n",
      "_________________________________________________________________\n",
      "attention_layer_1 (Attention (None, 64)                1064      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 87,748,685\n",
      "Trainable params: 87,748,685\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_model = build_gru_model(embedding_matrix=avg_pretrained_embeddings, embedding_size=EMBED_SIZE, \n",
    "                            max_len=MAX_LEN, max_features=MAX_FEATURES, gru_units=32)\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5141024124881741, 1: 36.45492662473794}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_y),\n",
    "                                                 train_y)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "class_weights[1] *= 2\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.4,\n",
    "                              patience=2, min_lr=0.00001)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=8, \n",
    "                           mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "callbacks = [reduce_lr, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15650 samples, validate on 1739 samples\n",
      "Epoch 1/20\n",
      "15650/15650 [==============================] - 53s 3ms/step - loss: 0.8379 - acc: 0.4127 - val_loss: 0.5245 - val_acc: 0.6343\n",
      "Epoch 2/20\n",
      "15650/15650 [==============================] - 50s 3ms/step - loss: 0.5718 - acc: 0.6927 - val_loss: 0.3991 - val_acc: 0.7453\n",
      "Epoch 3/20\n",
      "15650/15650 [==============================] - 50s 3ms/step - loss: 0.2403 - acc: 0.9208 - val_loss: 0.2031 - val_acc: 0.9563\n",
      "Epoch 4/20\n",
      "15650/15650 [==============================] - 50s 3ms/step - loss: 0.1166 - acc: 0.9703 - val_loss: 0.2981 - val_acc: 0.9689\n",
      "Epoch 5/20\n",
      "15650/15650 [==============================] - 50s 3ms/step - loss: 0.0711 - acc: 0.9777 - val_loss: 0.3116 - val_acc: 0.9620\n",
      "Epoch 6/20\n",
      "15650/15650 [==============================] - 50s 3ms/step - loss: 0.0294 - acc: 0.9918 - val_loss: 0.3711 - val_acc: 0.9701\n",
      "Epoch 7/20\n",
      "15650/15650 [==============================] - 50s 3ms/step - loss: 0.0211 - acc: 0.9943 - val_loss: 0.6554 - val_acc: 0.9816\n",
      "Epoch 8/20\n",
      "15650/15650 [==============================] - 50s 3ms/step - loss: 0.0159 - acc: 0.9965 - val_loss: 0.7643 - val_acc: 0.9833\n",
      "Epoch 9/20\n",
      "15650/15650 [==============================] - 50s 3ms/step - loss: 0.0130 - acc: 0.9966 - val_loss: 0.6490 - val_acc: 0.9845\n",
      "Epoch 10/20\n",
      "15650/15650 [==============================] - 50s 3ms/step - loss: 0.0131 - acc: 0.9973 - val_loss: 0.6902 - val_acc: 0.9839\n",
      "Epoch 11/20\n",
      "15650/15650 [==============================] - 50s 3ms/step - loss: 0.0127 - acc: 0.9974 - val_loss: 0.7071 - val_acc: 0.9839\n"
     ]
    }
   ],
   "source": [
    "history = gru_model.fit(train_X, train_y, batch_size=256, epochs=20, callbacks=callbacks,\n",
    "                        class_weight=class_weights, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Model Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model.save('./models/v3-jun19/model_files/cve_model_train75-jun19.h5')\n",
    "gru_model.save_weights('./models/v3-jun19/model_files/cve_model_train75-jun19_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5798/5798 [==============================] - 9s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "gru_model = keras.models.load_model('./models/v3-jun19/model_files/cve_model_train75-jun19.h5',\n",
    "                                        custom_objects={'AttentionLayer': AttentionLayer})\n",
    "pred_y = gru_model.predict([test_X], batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_yr = pred_y.ravel()\n",
    "pred_yl = [1 if prob > 0.3 else 0 for prob in pred_yr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5579,   61],\n",
       "       [  49,  109]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "confusion_matrix(y_true=test_y, y_pred=pred_yl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      5640\n",
      "          1       0.64      0.69      0.66       158\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=test_y, y_pred=pred_yl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
